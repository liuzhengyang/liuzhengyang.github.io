(window.webpackJsonp=window.webpackJsonp||[]).push([[265],{697:function(e,t,a){"use strict";a.r(t);var s=a(34),r=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"kafka-storage"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kafka-storage"}},[e._v("#")]),e._v(" kafka storage")]),e._v(" "),a("p",[e._v("今天我们主要讲解kafka如何接收producer发送消息、保存消息的。")]),e._v(" "),a("h2",{attrs:{id:"kafka是如何存储消息的"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kafka是如何存储消息的"}},[e._v("#")]),e._v(" kafka是如何存储消息的")]),e._v(" "),a("h3",{attrs:{id:"文件目录"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#文件目录"}},[e._v("#")]),e._v(" 文件目录")]),e._v(" "),a("p",[e._v("kafka broker将消息和相关的文件存储在server.properties配置文件中log.dirs的属性的文件夹中。")]),e._v(" "),a("p",[e._v("比如我们现在有一个名为test的topic，一个partition。则在log.dirs目录下会有test-0的目录，其中保存test消息的partition为0的日志文件。\n其他还可以看到__consumer_offsets-9这样的文件夹，是kafka内部保存consumer offset的消息的文件夹。")]),e._v(" "),a("div",{staticClass:"language-text line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("├── __consumer_offsets-9\n│   ├── 00000000000000000000.index\n│   ├── 00000000000000000000.log\n│   ├── 00000000000000000000.timeindex\n│   ├── leader-epoch-checkpoint\n│   └── partition.metadata\n├── cleaner-offset-checkpoint\n├── log-start-offset-checkpoint\n├── meta.properties\n├── recovery-point-offset-checkpoint\n├── replication-offset-checkpoint\n└── test-0\n    ├── 00000000000000000000.index\n    ├── 00000000000000000000.log\n    ├── 00000000000000000000.timeindex\n    ├── leader-epoch-checkpoint\n    └── partition.metadata\n")])]),e._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[e._v("1")]),a("br"),a("span",{staticClass:"line-number"},[e._v("2")]),a("br"),a("span",{staticClass:"line-number"},[e._v("3")]),a("br"),a("span",{staticClass:"line-number"},[e._v("4")]),a("br"),a("span",{staticClass:"line-number"},[e._v("5")]),a("br"),a("span",{staticClass:"line-number"},[e._v("6")]),a("br"),a("span",{staticClass:"line-number"},[e._v("7")]),a("br"),a("span",{staticClass:"line-number"},[e._v("8")]),a("br"),a("span",{staticClass:"line-number"},[e._v("9")]),a("br"),a("span",{staticClass:"line-number"},[e._v("10")]),a("br"),a("span",{staticClass:"line-number"},[e._v("11")]),a("br"),a("span",{staticClass:"line-number"},[e._v("12")]),a("br"),a("span",{staticClass:"line-number"},[e._v("13")]),a("br"),a("span",{staticClass:"line-number"},[e._v("14")]),a("br"),a("span",{staticClass:"line-number"},[e._v("15")]),a("br"),a("span",{staticClass:"line-number"},[e._v("16")]),a("br"),a("span",{staticClass:"line-number"},[e._v("17")]),a("br")])]),a("p",[e._v("根目录下包含各个TopicPartition文件夹和checkpoint等文件")]),e._v(" "),a("p",[e._v("文件夹名称为 {topic}-{partition}，例如topic为test的消息的partition为0的文件夹是test-0")]),e._v(" "),a("p",[e._v("TopicPartition文件夹内部的文件有")]),e._v(" "),a("ul",[a("li",[e._v("00000000000000000000.log文件，log文件是一系列的log消息的列表，每个列表由一个4byte的数据表示消息长度，然后是对应长度的消息byte。kafka会对log文件进行分段(segment)，每个分段文件的文件名是这个文件中的第一个消息的offset。分段策略是当文件大小超过1G（可以配置）时进行分段，可以避免大文件的查找写入性能下降问题。")]),e._v(" "),a("li",[e._v("00000000000000000000.index文件")]),e._v(" "),a("li",[e._v("00000000000000000000.timeindex文件")]),e._v(" "),a("li",[e._v("leader-epoch-checkpoint")]),e._v(" "),a("li",[e._v("partition.metadata文件，保存topic_id（是broker内部的id）和version")])]),e._v(" "),a("p",[e._v("根目录下的几个文件分别为")]),e._v(" "),a("ul",[a("li",[e._v("cleaner-offset-checkpoint")]),e._v(" "),a("li",[e._v("log-start-offset-checkpoint")]),e._v(" "),a("li",[e._v("meta.properties: 保存cluster.id, broker.id, version")]),e._v(" "),a("li",[e._v("recovery-point-offset-checkpoint")]),e._v(" "),a("li",[e._v("replication-offset-checkpoint")])]),e._v(" "),a("p",[e._v("上面我们还可以看到kafka内部使用的记录consumer offset的__consumer_offsets消息文件的文件夹。")]),e._v(" "),a("h2",{attrs:{id:"读写策略"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#读写策略"}},[e._v("#")]),e._v(" 读写策略")]),e._v(" "),a("p",[e._v("kafka不会直接写入磁盘，而是先写到操作系统buffer中，等待消息达到一定数量（log.flush.interval.messages配置）或buffer达到一定时间（log.flush.interval.ms/log.flush.scheduler.interval.ms配置）flush同步到磁盘（FileChannel.force())\n所以即使机器故障，最多只会丢失固定数量或固定时间的消息。")]),e._v(" "),a("p",[e._v("consumer拉取消息时，broker会查找对应的消息存在的segment文件，因为segment文件的文件名是表示第一个消息的offset，\nTODO broker通过segment文件名内存的二分查找就可以额定位到消息所在的segment文件。\n文件内部的位置怎么定位？TODO")]),e._v(" "),a("h2",{attrs:{id:"清理策略"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#清理策略"}},[e._v("#")]),e._v(" 清理策略")]),e._v(" "),a("p",[e._v("旧消息需要清理来避免文件过大导致磁盘占满，清理有删除和压缩两种方式(默认删除），判断是否要清理有时间和消息大小两个策略。")]),e._v(" "),a("p",[e._v("时间策略: 按照segment中消息的最大的时间戳计算，如果距离当前时间超过了指定时间(log.retention.hours默认168小时或log.retention.minutes或log.retention.ms)，则会删除这个segment文件。")]),e._v(" "),a("p",[e._v("大小策略: 默认关闭，如果开启后，broker当检测到某个partition下所有的segment文件的总大小超过一定的值（可配置），则会删除最旧的segment文件。")]),e._v(" "),a("h3",{attrs:{id:"压缩"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#压缩"}},[e._v("#")]),e._v(" 压缩")]),e._v(" "),a("p",[e._v("对于一些比较重要的数据，删除不是一种合适的策略，因为可能出现数据丢失，所以kafka提供了一种压缩的策略。\n压缩策略把消息按照key维度进行保存，只保存一个key最后一条记录，对于状态变更类消息比较友好，比如kafka内部记录offset的__consumer_offsets消息，\n里面保存的就是一个group下某个topic partition的消费的offset，理论上只需要保存最后的offset，所以__consumer_offsets使用这种策略，如果使用删除策略会有可能导致最后的offset记录丢失。")]),e._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"img.png","data-src":"/assets/images/kafka/kafka-compact.png",loading:"lazy"}})]),e._v(" "),a("h2",{attrs:{id:"kafka-broker-处理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kafka-broker-处理"}},[e._v("#")]),e._v(" Kafka Broker 处理")]),e._v(" "),a("ol",[a("li",[e._v("检查topicPartition是否有权限、是否存在")]),e._v(" "),a("li",[e._v("调用replicaManager.appendRecords 保存消息\n"),a("ul",[a("li",[e._v("appendToLocalLog 保存log到本地\n"),a("ul",[a("li",[e._v("Partition.appendRecordsToLeader")])])])])])]),e._v(" "),a("p",[e._v("对于acks=0,1,-1(all)，broker是如何处理的？")]),e._v(" "),a("p",[e._v("delayed operation")]),e._v(" "),a("h2",{attrs:{id:"fetch-message时的zero-copy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fetch-message时的zero-copy"}},[e._v("#")]),e._v(" fetch message时的zero-copy")]),e._v(" "),a("h2",{attrs:{id:"todo"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#todo"}},[e._v("#")]),e._v(" TODO")]),e._v(" "),a("h2",{attrs:{id:"log文件的磁盘写入时机"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#log文件的磁盘写入时机"}},[e._v("#")]),e._v(" Log文件的磁盘写入时机")]),e._v(" "),a("p",[e._v("写入时调用FileChannel.write，写入到操作系统的pagecache内存中。然后定期调用flush，同步到磁盘。")]),e._v(" "),a("p",[e._v("刷新策略如下")]),e._v(" "),a("p",[e._v("时间策略: 如果一条消息在pagecache中停留时间超过"),a("code",[e._v("log.flush.interval.ms")]),e._v("（如果没有设置会使用"),a("code",[e._v("log.flush.scheduler.interval.ms")]),e._v("）")]),e._v(" "),a("p",[e._v("checkpoint")]),e._v(" "),a("p",[e._v("index\ntimeindex")])])}),[],!1,null,null,null);t.default=r.exports}}]);